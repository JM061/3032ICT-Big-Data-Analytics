setwd("C:/Users/Jacob/Documents/UNI CODE/3032ICT-Big-Data-Analytics/3032ICT Assignment 1")
load("~/UNI CODE/3032ICT-Big-Data-Analytics/3032ICT Assignment 1/Final Data/Youtube Data/Youtube Semantic Graphs In Progress.RData")
load("~/UNI CODE/3032ICT-Big-Data-Analytics/3032ICT Assignment 1/Final Data/Youtube Data/YouTube Semantic Data.RData")
View(yt_bigrams)
View(yt_bigrams)
View(yt_data)
View(yt_data)
View(yt_clean_df)
# Clean the text with help from the textclean package
yt_clean_text <- yt_data$Comment |>
replace_url() |>
replace_html() |>
replace_non_ascii() |> # ` vs '
replace_word_elongation() |>
replace_internet_slang() |>
replace_contraction() |>
removeNumbers() |>
removePunctuation()   |>
replace_emoji() |> # optional
replace_emoticon() |> # optional
yt_clean_df <- data.frame(yt_clean_text)
library(vosonSML)
library(igraph)
library(dplyr)
library(textclean)
library(tidytext)
library(tidyr)
library(vosonSML)
library(igraph)
library(dplyr)
library(textclean)
library(tidytext)
library(tidyr)
library(tm)
library(tidyr)
library(tm)
# Clean the text with help from the textclean package
yt_clean_text <- yt_data$Comment |>
replace_url() |>
replace_html() |>
replace_non_ascii() |> # ` vs '
replace_word_elongation() |>
replace_internet_slang() |>
replace_contraction() |>
removeNumbers() |>
removePunctuation()   |>
replace_emoji() |> # optional
replace_emoticon() |> # optional
yt_clean_df <- data.frame(yt_clean_text)
# Clean the text with help from the textclean package
yt_clean_text <- yt_data$Comment |>
replace_url() |>
replace_html() |>
replace_non_ascii() |> # ` vs '
replace_word_elongation() |>
replace_internet_slang() |>
replace_contraction() |>
removeNumbers() |>
removePunctuation()   |>
replace_emoji() |> # optional
replace_emoticon() #|> # optional
yt_clean_df[1:10]
library(vosonSML)
library(dplyr)
library(tidyr)
library(tidytext)
library(textclean)
library(tm)
library(ggplot2)
library(igraph)
library(ggplot2)
yt_clean_df[1:10]
yt_clean_text[1:10]
yt_clean_text <- yt_clean_text[complete.cases(yt_clean_text), ]
yt_data_no_na <- yt_data[complete.cases(yt_data), ]
View(yt_data_no_na)
# TD Matrix Creation
#Youtube
yt_clean_text <- yt_data_no_na$Comment |>
replace_url() |>
replace_html() |>
replace_non_ascii() |> # ` vs '
replace_word_elongation() |>
replace_internet_slang() |>
replace_contraction() |>
removeNumbers() |>
removePunctuation()   |>
replace_emoji() |> # optional
replace_emoticon() #|> # optional
yt_clean_df <- data.frame(yt_clean_text)
yt_clean_text[1:10]
View(yt_clean_df)
yt_data_no_na <- yt_clean_df[complete.cases(yt_clean_df), ]
view(yt_data_no_na)
yt_data_no_na
yt_clean_text[1:10]
table(yt_clean_text)
yt_clean_text[1:10]
yt_data_no_na[1:10]
#  Tokenisation: split into bigrams (two neighbouring words)
yt_bigrams <- yt_data_no_na |> unnest_tokens(output = bigram,
input = yt_clean_text,
token = "ngrams",
n = 2)
yt_data_no_na <- yt_clean_df[complete.cases(yt_clean_df), ]
#  Tokenisation: split into bigrams (two neighbouring words)
yt_bigrams <- yt_data_no_na |> unnest_tokens(output = bigram,
input = yt_clean_text,
token = "ngrams",
n = 2)
library(vosonSML)
library(dplyr)
library(tidyr)
library(tidytext)
library(textclean)
library(tm)
library(ggplot2)
library(igraph)
#  Tokenisation: split into bigrams (two neighbouring words)
yt_bigrams <- yt_data_no_na |> unnest_tokens(output = bigram,
input = yt_clean_text,
token = "ngrams",
n = 2)
#  Tokenisation: split into bigrams (two neighbouring words)
yt_bigrams <- yt_clean_df |> unnest_tokens(output = bigram,
input = yt_clean_text,
token = "ngrams",
n = 2)
yt_bigrams_table <- yt_bigrams |>
count(bigram, sort = TRUE) |>
separate(bigram, c("left", "right"))
yt_bigrams_nostops <- yt_bigrams_table |>
anti_join(stop_words, join_by(left == word)) |>
anti_join(stop_words, join_by(right == word))
# Remove rows that have 'NA'
yt_bigrams_nostops <- yt_bigrams_nostops[complete.cases(yt_bigrams_nostops), ]
# If you have lots of bigrams, keep only those that occur at least X times (here X is 2)
yt_bigrams_nostops <- yt_bigrams_nostops |> filter(n >= 2)
yt_bigram_graph <- graph_from_data_frame(yt_bigrams_nostops, directed = FALSE)
yt_bigram_graph <- simplify(yt_bigram_graph) # remove loops and multiple edges
View(yt_bigrams_nostops)
View(yt_clean_df)
View(yt_bigrams)
View(yt_bigram_graph)
write_graph(yt_bigram_graph, file = "Test_YouTubeSemantic.graphml", format = "graphml")
save.image("~/UNI CODE/3032ICT-Big-Data-Analytics/3032ICT Assignment 1/Final Data/youtubeTDMATRIX In progress.RData")
